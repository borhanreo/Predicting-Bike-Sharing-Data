def forward_pass_train(self, X):
        ''' Implement forward pass here 
         
            Arguments
            ---------
            X: features batch

        '''
        inputs = np.array(X, ndmin=2).T
        #print(inputs.shape[0])
        #print(inputs.shape[1])
        #targets = np.array(targets_list, ndmin = 2).T

        #### Implement the forward pass here ####
        ### Forward pass ###
        # TODO: Hidden layer - Replace these values with your calculations.
        hidden_inputs = np.dot(self.weights_input_to_hidden.T, X.T) # signals into hidden layer
        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer

        # TODO: Output layer - Replace these values with your calculations.
        final_inputs = np.dot(self.weights_hidden_to_output.T, hidden_outputs) # signals into final output layer
        final_outputs = final_inputs # signals from final output layer
        
        #print(final_outputs.shape[0])
        #print(final_outputs.shape[1])
        #print(hidden_outputs.shape[0])
        #print(hidden_outputs.shape[1])
        
        return final_outputs, hidden_outputs

    def backpropagation(self, final_outputs, hidden_outputs, X, y, delta_weights_i_h, delta_weights_h_o):
        ''' Implement backpropagation
         
            Arguments
            ---------
            final_outputs: output from forward pass
            y: target (i.e. label) batch
            delta_weights_i_h: change in weights from input to hidden layers
            delta_weights_h_o: change in weights from hidden to output layers

        '''
         #### Implement the backward pass here ####
        ### Backward pass ###

        # TODO: Output error - Replace this value with your calculations.
        error = y - final_outputs # Output layer error is the difference between desired target and actual output.
        output_error_term = error * final_outputs * (1 - final_outputs)
        
        
        # TODO: Calculate the hidden layer's contribution to the error
        hidden_error = np.dot(self.weights_hidden_to_output.T, error)
        hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)
        
        # Weight step (input to hidden)
        #print(delta_weights_i_h.shape[0])
        #print(delta_weights_i_h.shape[1])
        
        delta_weights_i_h += np.dot(X, hidden_error_term.T)
        
        #np.dot(X[:, None], hidden_error_term[:, None].T)
        
        # Weight step (hidden to output)        
        delta_weights_h_o += np.dot(hidden_outputs.T[:, None], error[:, None])
        
        # output_error_term[:, None])
        
        return delta_weights_i_h, delta_weights_h_o
        

    def update_weights(self, delta_weights_i_h, delta_weights_h_o, n_records):
        ''' Update weights on gradient descent step
         
            Arguments
            ---------
            delta_weights_i_h: change in weights from input to hidden layers
            delta_weights_h_o: change in weights from hidden to output layers
            n_records: number of records

        '''
        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records  # update hidden-to-output weights with gradient descent step
        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records  # update input-to-hidden weights with gradient descent step
